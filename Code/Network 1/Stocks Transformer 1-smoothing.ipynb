{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6af7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "from scipy.interpolate import LSQUnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb44225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses same model as in Stocks Transformer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7826cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim): \n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) \n",
    "        C = torch.randn((batch_size, self.input_size, self.embed_dim)) \n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                C[i][j]*=x[i][j]\n",
    "        return C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_model_dim\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, self.embed_dim) \n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/self.embed_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        \n",
    "        pe = pe.unsqueeze(0) \n",
    "        self.register_buffer('pe', pe) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x *= math.sqrt(self.embed_dim) \n",
    "        seq_len = x.size(1)\n",
    "        x += torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d72db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads) \n",
    "        \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False) \n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False) \n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False) \n",
    "        \n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "        \n",
    "    def forward(self, key, query, value, mask=None):\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim) \n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
    "\n",
    "        k = self.key_matrix(key).transpose(1, 2) \n",
    "        q = self.query_matrix(query).transpose(1, 2)   \n",
    "        v = self.value_matrix(value).transpose(1, 2) \n",
    "\n",
    "        k_adjusted = k.transpose(-1, -2) \n",
    "\n",
    "        product = torch.matmul(q, k_adjusted)\n",
    "\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "\n",
    "        product = product / math.sqrt(self.single_head_dim) \n",
    "\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "        scores = torch.matmul(scores, v) \n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads) \n",
    "\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_value, expansion_factor = 4, n_heads = 8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(embed_dim, expansion_factor * embed_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(expansion_factor * embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_value)\n",
    "        self.dropout2 = nn.Dropout(dropout_value)\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        attention_out = self.attention(key, query, value)\n",
    "        attention_residual_out = attention_out + query\n",
    "        norm1_out = self.dropout1(self.norm1(attention_out)) \n",
    "        \n",
    "        feed_fwd_out = self.feed_forward(norm1_out)\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n",
    "        \n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e574f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8): \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = Embedding(input_size, embed_dim) \n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, dropout_value, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_value, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, dropout_value, expansion_factor, n_heads)\n",
    "        \n",
    "    def forward(self, key, x, value, mask):\n",
    "        attention = self.attention(x, x, x, mask=mask)\n",
    "        x = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(key, x, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_input_size, embed_dim, seq_len, target_output_size, dropout_value, num_layers=2, expansion_factor=4, n_heads=8): ###batch size after target output\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(decoder_input_size, embed_dim) \n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "        self.fst_attention = DecoderBlock(embed_dim, dropout_value, expansion_factor=4, n_heads=8)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, dropout_value, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc1_out = nn.Linear(embed_dim, target_output_size)\n",
    "        self.fc2_out = nn.Linear(decoder_input_size, target_output_size)\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        \n",
    "    def forward(self, x, enc_out, mask):\n",
    "        x = self.embedding(x) \n",
    "        x = self.position_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask=None)\n",
    "        \n",
    "        out = self.fc1_out(x)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.fc2_out(out)\n",
    "        out = torch.squeeze(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, input_size, decoder_input_size, target_output_size, seq_length,num_layers=2, dropout_value=0.2, expansion_factor=4, n_heads=8): ###batch_size, after target output\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.decoder_input_size = decoder_input_size\n",
    "        \n",
    "        self.encoder = TransformerEncoder(seq_length, input_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads) ###batch_size=batch_size, after embed dim\n",
    "        self.decoder = TransformerDecoder(decoder_input_size, embed_dim, seq_length, target_output_size, dropout_value, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads) ###batch_size=batch_size, after target output\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(batch_size, 1, trg_len, trg_len) \n",
    "        return trg_mask\n",
    "    \n",
    "    def decode(self, src, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "        \n",
    "        out = trg\n",
    "        for i in range(seq_len):\n",
    "            out = self.decoder(out, enc_out, trg_mask)\n",
    "            \n",
    "            out = out[:, -1, :]\n",
    "            \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out, axis=0)\n",
    "            \n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src) \n",
    "        \n",
    "        outputs = self.decoder(trg, enc_out, trg_mask) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15256046",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "decoder_input_size = 9\n",
    "target_output_size = 1\n",
    "num_layers = 6\n",
    "seq_length = 10 \n",
    "batch_size = 16\n",
    "dropout_value = 0.2\n",
    "num_training_stocks = 4\n",
    "\n",
    "\n",
    "model = Transformer(embed_dim=32, input_size=input_size, \n",
    "                    decoder_input_size=decoder_input_size, target_output_size=target_output_size, seq_length=seq_length,\n",
    "                    num_layers=num_layers, dropout_value=dropout_value, expansion_factor=4, n_heads=8) ###batch_size=batch_size, after num layers\n",
    "\n",
    "current_path = os.path.join('')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59feb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, inputs, labels, transform=None, target_transform=None):\n",
    "        self.labels = labels\n",
    "        self.inputs = inputs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inpt = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            inpt = self.transform(inpt)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inpt, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "mypath = os.path.join(current_path, \"..\", \"..\", \"Data\", \"Network 1 data\", 'Stocks')\n",
    "stocks = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "Xtr, Ytr = [], []\n",
    "Xval, Yval = [], []\n",
    "Xte, Yte = [], []\n",
    "\n",
    "block_size = input_size\n",
    "\n",
    "\n",
    "def build_dataset(prices):\n",
    "            X, Y = [], []\n",
    "            if len(prices)<block_size+1:\n",
    "                return [], []\n",
    "            for i in range(len(prices)-block_size):\n",
    "                X.append(prices[i:i+block_size])\n",
    "                Y.append(prices[i+block_size])\n",
    "            X = torch.tensor(X)\n",
    "            Y = torch.tensor(Y)\n",
    "            return X, Y\n",
    "\n",
    "#choose which method of smoothing to use\n",
    "method = 'mov'\n",
    "smooth_const = 5\n",
    "        \n",
    "#create the datasets\n",
    "all_train_prices = [0]\n",
    "all_smooth_prices = [0]\n",
    "for stock in stocks[1:num_training_stocks+1]:\n",
    "    data = open(os.path.join(mypath, stock)).read().splitlines()[1:]\n",
    "\n",
    "\n",
    "    year_lookup = {}\n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        year = int(lst[0][:4])\n",
    "        closing = float(lst[4])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [closing]\n",
    "        else:\n",
    "            year_lookup[year].append(closing)\n",
    "    \n",
    "    #build dataset\n",
    "    prices = []\n",
    "    train_prices = []\n",
    "    validation_prices = []\n",
    "    test_prices = []\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    years = list(year_lookup.keys())\n",
    "    if stock == stocks[num_training_stocks]:\n",
    "        for year in years:\n",
    "            test_prices += year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "    else:\n",
    "        training_cut_off = years[math.ceil(0.75*len(years))-1]\n",
    "        for year in range(years[0], training_cut_off):\n",
    "            train_prices+=year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "\n",
    "        for year in range(training_cut_off, years[-1]):\n",
    "            validation_prices+=year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "        \n",
    "    mean = np.mean(prices)\n",
    "    std = np.std(prices)\n",
    "    \n",
    "    train_prices = (train_prices-mean)/std\n",
    "    validation_prices = (test_prices-mean)/std\n",
    "    test_prices = (test_prices-mean)/std\n",
    "    \n",
    "    #moving average\n",
    "    if method == 'mov':\n",
    "        smooth_prices = [price for price in train_prices[:smooth_const-1]]\n",
    "        for i in range(smooth_const, len(train_prices)+1):\n",
    "            smooth_prices.append(sum(train_prices[i-smooth_const:i])/smooth_const)\n",
    "        \n",
    "\n",
    "    #exponential moving average\n",
    "    if method == 'exp':\n",
    "        if len(train_prices) > 0:\n",
    "            smooth_prices = [train_prices[0]]\n",
    "            for i in range(1, len(train_prices)):\n",
    "                smooth_prices.append(smooth_const*train_prices[i]+(1-smooth_const)*smooth_prices[i-1])\n",
    "            \n",
    "    \n",
    "    \n",
    "    #adjusted moving average\n",
    "    if method == 'adj':\n",
    "        smooth_prices = [price for price in train_prices[:smooth_const-1]]\n",
    "        for i in range(smooth_const, len(train_prices)+1):\n",
    "            smooth_prices.append(sum(train_prices[i-smooth_const:i])/smooth_const)\n",
    "\n",
    "        def bias_calc(lst):\n",
    "            bias = 0\n",
    "            count = 0\n",
    "            for j in range(1, len(lst)):\n",
    "                if (train_prices[j]-lst[j])*(train_prices[j-1]-lst[j-1]) > 0:\n",
    "                    count+=1\n",
    "                    bias+=count**2\n",
    "                else:\n",
    "                    count = 0\n",
    "            return bias\n",
    "\n",
    "        bias1 = {}\n",
    "        for alpha in [i/1000 for i in range(31)]:\n",
    "            bias = [bias_calc(smooth_prices), bias_calc(smooth_prices)]\n",
    "            Beta = 0\n",
    "            sign = 1\n",
    "            end = True\n",
    "            for step in [0.1, 0.01, 0.001]:\n",
    "                while bias[1] <= bias[0]:\n",
    "                    Beta += sign*step\n",
    "                    adjusted_prices = [price for price in smooth_prices[0:math.ceil(smooth_const*1.5)]]\n",
    "                    for i in range(math.ceil(smooth_const*1.5) + 1, len(smooth_prices)):\n",
    "                        if abs((smooth_prices[i]-smooth_prices[i-1])/smooth_prices[i-1]) > alpha:\n",
    "                            adjusted_prices.append(smooth_prices[i]+Beta*(smooth_prices[i]-smooth_prices[i-1]))\n",
    "                            end = False\n",
    "                        else:\n",
    "                            adjusted_prices.append(smooth_prices[i])\n",
    "                    bias[0] = bias[1]\n",
    "                    bias[1] = bias_calc(adjusted_prices)\n",
    "                    if end:\n",
    "                        break\n",
    "                bias[0] = bias[1]\n",
    "                if end:\n",
    "                    break\n",
    "                sign*=-1\n",
    "            bias1[alpha] = [Beta-0.001, bias[0]]\n",
    "\n",
    "        best_bias = bias1[0.001][1]\n",
    "        best_alpha = 0.001\n",
    "        best_beta = 0\n",
    "        for alpha in bias1.keys():\n",
    "            if bias1[alpha][1] < best_bias:\n",
    "                best_alpha = alpha\n",
    "                best_beta = bias1[alpha][0]\n",
    "                best_bias = bias1[alpha][1]\n",
    "\n",
    "        adj_smooth_prices = [price for price in smooth_prices]\n",
    "        for i in range(math.ceil(smooth_const*1.5) + 1, len(smooth_prices)):\n",
    "            if abs((smooth_prices[i]-smooth_prices[i-1])/smooth_prices[i-1]) > alpha:\n",
    "                adj_smooth_prices[i] += best_beta*(smooth_prices[i]-smooth_prices[i-1])\n",
    "        smooth_prices = adj_smooth_prices\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    #spline smoothing\n",
    "    if method == 'spline':\n",
    "        if len(train_prices) > 1:\n",
    "            num_datapoints = smooth_const\n",
    "            num_knots = max(0, math.floor(len(train_prices)/num_datapoints-1))\n",
    "            time = np.arange(len(train_prices))\n",
    "\n",
    "            knot_indices = np.linspace(num_datapoints, len(time) - 1-num_datapoints, num_knots).astype(int)\n",
    "            knots = time[knot_indices]\n",
    "\n",
    "            cs = LSQUnivariateSpline(time, train_prices, k=3, t=knots)\n",
    "            smooth_prices = cs(time)\n",
    "\n",
    "    Xtr += build_dataset(smooth_prices)[0]\n",
    "    Ytr += build_dataset(smooth_prices)[1]\n",
    "    Xval += build_dataset(validation_prices)[0]\n",
    "    Yval += build_dataset(validation_prices)[1]\n",
    "    Xte, Yte = build_dataset(test_prices)\n",
    "    \n",
    "    all_train_prices.extend(train_prices)\n",
    "    all_smooth_prices.extend(smooth_prices)\n",
    "\n",
    "training_data = CustomDataset(Xtr, Ytr)\n",
    "val_data = CustomDataset(Xval, Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d46f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load smoothed prices into dicts to calculate bias and mse for each method\n",
    "stocks = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "def add_to_dict(key, value, dct):\n",
    "    if key in dct:\n",
    "        dct[key] = np.concatenate((dct[key],value))\n",
    "    else:\n",
    "        dct[key] = value\n",
    "\n",
    "all_train_prices = []\n",
    "\n",
    "all_mov_smooth_prices = {}\n",
    "all_centmov_smooth_prices = {}\n",
    "all_exp_smooth_prices = {}\n",
    "all_spline_smooth_prices = {}\n",
    "\n",
    "for stock in stocks[1:num_training_stocks+1]:\n",
    "    data = open(os.path.join(mypath, stock)).read().splitlines()[1:]\n",
    "\n",
    "\n",
    "    year_lookup = {}\n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        year = int(lst[0][:4])\n",
    "        closing = float(lst[4])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [closing]\n",
    "        else:\n",
    "            year_lookup[year].append(closing)\n",
    "    \n",
    "    #build dataset\n",
    "    prices = []\n",
    "    train_prices = []\n",
    "    validation_prices = []\n",
    "    test_prices = []\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    years = list(year_lookup.keys())\n",
    "    if stock == stocks[num_training_stocks]:\n",
    "        for year in years:\n",
    "            test_prices += year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "    else:\n",
    "        training_cut_off = years[math.ceil(0.75*len(years))-1]\n",
    "        for year in range(years[0], training_cut_off):\n",
    "            train_prices+=year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "\n",
    "        for year in range(training_cut_off, years[-1]):\n",
    "            validation_prices+=year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "    \n",
    "    #moving average\n",
    "    for sc in [3, 5, 10, 20, 50]:\n",
    "        smooth_const = sc\n",
    "        mov_smooth_prices = [price for price in train_prices[:smooth_const-1]]\n",
    "        for i in range(smooth_const, len(train_prices)+1):\n",
    "            mov_smooth_prices.append(sum(train_prices[i-smooth_const:i])/smooth_const)\n",
    "        add_to_dict(sc, mov_smooth_prices, all_mov_smooth_prices)\n",
    "        \n",
    "    #centered moving average\n",
    "    for sc in [3, 5, 10, 20]:\n",
    "        smooth_const = sc\n",
    "        centmov_smooth_prices = [price for price in train_prices[:math.ceil(smooth_const/2)-1]]\n",
    "        for i in range(math.ceil(smooth_const/2), len(train_prices)+1-math.floor(smooth_const/2)):\n",
    "            centmov_smooth_prices.append(sum(train_prices[i-math.ceil(smooth_const/2):i+math.floor(smooth_const/2)])/smooth_const)\n",
    "        centmov_smooth_prices.extend(train_prices[-math.floor(smooth_const/2):])\n",
    "        add_to_dict(sc, centmov_smooth_prices, all_centmov_smooth_prices)  \n",
    "    \n",
    "    #exponential moving average\n",
    "    if len(train_prices) > 0:\n",
    "        for sc in [0.1, 0.15, 0.2, 0.25]:\n",
    "            smooth_const = sc\n",
    "            exp_smooth_prices = [train_prices[0]]\n",
    "            for i in range(1, len(train_prices)):\n",
    "                exp_smooth_prices.append(smooth_const*train_prices[i]+(1-smooth_const)*exp_smooth_prices[i-1])\n",
    "            add_to_dict(sc, exp_smooth_prices, all_exp_smooth_prices)\n",
    "\n",
    "    #spline smoothing\n",
    "    if len(train_prices) > 1:\n",
    "        for n in [5, 10, 15, 20]:\n",
    "            num_datapoints = n\n",
    "            num_knots = max(0, math.floor(len(train_prices)/num_datapoints-1))\n",
    "            time = np.arange(len(train_prices))\n",
    "\n",
    "            knot_indices = np.linspace(num_datapoints, len(time) - 1-num_datapoints, num_knots).astype(int)\n",
    "            knots = time[knot_indices]\n",
    "\n",
    "            cs = LSQUnivariateSpline(time, train_prices, k=3, t=knots)\n",
    "            spline_smooth_prices = cs(time)\n",
    "            add_to_dict(n, spline_smooth_prices, all_spline_smooth_prices)\n",
    "\n",
    "    #all_train_prices.extend(train_prices)\n",
    "    #all_smooth_prices.extend(smooth_prices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa51469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates bias values\n",
    "def bias_calc(lst):\n",
    "    bias = 0\n",
    "    count = 0\n",
    "    for j in range(1, len(lst)):\n",
    "        if (all_train_prices[j]-lst[j])*(all_train_prices[j-1]-lst[j-1]) > 0:\n",
    "            count+=1\n",
    "            bias+=count**2\n",
    "        else:\n",
    "            count = 0\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5d051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates dict for adjusted moving average \n",
    "all_adjmov_smooth_prices = {}\n",
    "Betas = {}\n",
    "for key in all_mov_smooth_prices.keys():\n",
    "    bias1 = {}\n",
    "    for alpha in [i/1000 for i in range(21)]:\n",
    "        bias = [bias_calc(all_mov_smooth_prices[key]), bias_calc(all_mov_smooth_prices[key])]\n",
    "        Beta = 0\n",
    "        sign = 1\n",
    "        for step in [0.1, 0.01, 0.001]:\n",
    "            while bias[1] <= bias[0]:\n",
    "                Beta += sign*step\n",
    "                adjusted_prices = [price for price in all_mov_smooth_prices[key][0:math.ceil(key*1.5)]]\n",
    "                for i in range(math.ceil(key*1.5) + 1, len(list(all_mov_smooth_prices[key]))):\n",
    "                    if abs(all_mov_smooth_prices[key][i]-all_mov_smooth_prices[key][i-1])/all_mov_smooth_prices[key[i-1]] > alpha:\n",
    "                        adjusted_prices.append(all_mov_smooth_prices[key][i]+Beta*(all_mov_smooth_prices[key][i]-all_mov_smooth_prices[key][i-1]))\n",
    "                    else:\n",
    "                        adjusted_prices.append(all_mov_smooth_prices[key][i])\n",
    "                bias[0] = bias[1]\n",
    "                bias[1] = bias_calc(adjusted_prices)\n",
    "            bias[0] = bias[1]\n",
    "            sign*=-1\n",
    "        if alpha not in bias1.keys():\n",
    "            bias1[alpha] = [Beta-0.001, bias[0]]\n",
    "        elif bias[0] < bias1[alpha][1]:\n",
    "            bias1[alpha] = [Beta-0.001, bias[0]]\n",
    "    \n",
    "    best_bias = bias1[0.001][1]\n",
    "    best_alpha = 0.001\n",
    "    best_beta = 0\n",
    "    for alpha in bias1.keys():\n",
    "        if bias1[alpha][1] < best_bias:\n",
    "            best_alpha = alpha\n",
    "            best_beta = bias1[alpha][0]\n",
    "        \n",
    "    Betas[key] = [best_alpha, best_beta]\n",
    "    smooth_const = key\n",
    "    adj_smooth_prices = [price for price in all_mov_smooth_prices[key]]\n",
    "    for i in range(math.ceil(key*1.5) + 1, len(list(all_mov_smooth_prices[key]))):\n",
    "        if abs(all_mov_smooth_prices[key][i]-all_mov_smooth_prices[key][i-1])/all_mov_smooth_prices[key[i-1]] > alpha:\n",
    "            adj_smooth_prices[i] += best_beta*(all_mov_smooth_prices[key][i]-all_mov_smooth_prices[key][i-1])\n",
    "    add_to_dict(key, adj_smooth_prices, all_adjmov_smooth_prices)\n",
    "print(Betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the graph of each smoothing method\n",
    "dicts = [all_mov_smooth_prices, all_centmov_smooth_prices, all_adjmov_smooth_prices, all_exp_smooth_prices, all_spline_smooth_prices]\n",
    "names = [\"mov\", \"centmov\", \"adjmov\", \"exp\", \"spline\"]\n",
    "\n",
    "for i in range(len(dicts)):\n",
    "    smooth = dicts[i]\n",
    "    for key in smooth.keys():\n",
    "        print(names[i], key)\n",
    "        plt.plot(all_train_prices[:1000], alpha=0.7, label='real data')\n",
    "        plt.plot(smooth[key][:1000], label='smoothed values')\n",
    "        plt.xlabel('Days', fontsize=15)\n",
    "        plt.ylabel('Closing Price', fontsize=15)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9694cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate the MSE for each method\n",
    "for i in range(len(dicts)):\n",
    "    smooth = dicts[i]\n",
    "    for key in smooth.keys():\n",
    "        print(names[i], key)\n",
    "        sum1=0\n",
    "        for j in range(1000):\n",
    "            sum1 += (all_train_prices[j]-smooth[key][j])**2\n",
    "        sum1/=len(all_train_prices)\n",
    "        print(sum1*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583e10a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate the bias values for each method\n",
    "for i in range(len(dicts)):\n",
    "    smooth = dicts[i]\n",
    "    for key in smooth.keys():\n",
    "        print(names[i], key)\n",
    "        print(bias_calc(smooth[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e18fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "train_losses, val_losses = [], []\n",
    "for e in range(1,epochs):\n",
    "    start_time = time.time()\n",
    "    tot_train_loss = 0\n",
    "    model.train()\n",
    "    for x, y in trainloader:\n",
    "        if len(y) != batch_size:\n",
    "            continue\n",
    "        trg = x[:, 1:]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(x, trg)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_train_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        tot_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, y in valloader:\n",
    "                if len(y) != batch_size:\n",
    "                    continue\n",
    "                trg = x[:, 1:]\n",
    "                outputs = model(x, trg)\n",
    "                loss = criterion(outputs, y)\n",
    "                tot_val_loss += loss.item()\n",
    "\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        val_loss = tot_val_loss / len(valloader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "              \"Test Loss: {:.3f}.. \".format(val_loss), \n",
    "              \"Time taken: {:.3f}..\".format(time.time()-start_time))\n",
    "        \n",
    "    if e%10 == 0:\n",
    "        torch.save(model.state_dict(), 'smoothcheckpoint' + str(e) + '.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
