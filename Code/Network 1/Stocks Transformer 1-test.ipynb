{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToTensor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b03088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses same model as in Stocks Transformer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7826cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim): \n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) \n",
    "        C = torch.randn((batch_size, self.input_size, self.embed_dim)) \n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                C[i][j]*=x[i][j]\n",
    "        return C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_model_dim\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, self.embed_dim) \n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/self.embed_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        \n",
    "        pe = pe.unsqueeze(0) \n",
    "        self.register_buffer('pe', pe) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x *= math.sqrt(self.embed_dim) \n",
    "        seq_len = x.size(1)\n",
    "        x += torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d72db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads) \n",
    "        \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False) \n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False) \n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False) \n",
    "        \n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "        \n",
    "    def forward(self, key, query, value, mask=None):\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim) \n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
    "        \n",
    "        k = self.key_matrix(key).transpose(1, 2) \n",
    "        q = self.query_matrix(query).transpose(1, 2)   \n",
    "        v = self.value_matrix(value).transpose(1, 2) \n",
    "\n",
    "        k_adjusted = k.transpose(-1, -2) \n",
    "\n",
    "        product = torch.matmul(q, k_adjusted)\n",
    "\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "\n",
    "        product = product / math.sqrt(self.single_head_dim) \n",
    "        scores = F.softmax(product, dim=-1)\n",
    "        scores = torch.matmul(scores, v) \n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads) \n",
    "\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_value, expansion_factor = 4, n_heads = 8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(embed_dim, expansion_factor * embed_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(expansion_factor * embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_value)\n",
    "        self.dropout2 = nn.Dropout(dropout_value)\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        attention_out = self.attention(key, query, value)\n",
    "        attention_residual_out = attention_out + query\n",
    "        norm1_out = self.dropout1(self.norm1(attention_out)) \n",
    "        \n",
    "        feed_fwd_out = self.feed_forward(norm1_out)\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n",
    "        \n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e574f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8): \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = Embedding(input_size, embed_dim) \n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, dropout_value, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_value, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, dropout_value, expansion_factor, n_heads)\n",
    "        \n",
    "    def forward(self, key, x, value, mask):\n",
    "        attention = self.attention(x, x, x, mask=mask)\n",
    "        x = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(key, x, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_input_size, embed_dim, seq_len, target_output_size, dropout_value, num_layers=2, expansion_factor=4, n_heads=8): ###batch size after target output\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(decoder_input_size, embed_dim) \n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "        self.fst_attention = DecoderBlock(embed_dim, dropout_value, expansion_factor=4, n_heads=8)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, dropout_value, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc1_out = nn.Linear(embed_dim, 1)\n",
    "        self.fc2_out = nn.Linear(decoder_input_size, target_output_size)\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        \n",
    "    def forward(self, x, enc_out, mask):\n",
    "        x = self.embedding(x) \n",
    "        x = self.position_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask=None)\n",
    "        \n",
    "        out = self.fc1_out(x)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.fc2_out(out)\n",
    "        out = torch.squeeze(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, input_size, decoder_input_size, target_output_size, seq_length,num_layers=2, dropout_value=0.2, expansion_factor=4, n_heads=8): \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.decoder_input_size = decoder_input_size\n",
    "        \n",
    "        self.encoder = TransformerEncoder(seq_length, input_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads) \n",
    "        self.decoder = TransformerDecoder(decoder_input_size, embed_dim, seq_length, target_output_size, dropout_value, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(batch_size, 1, trg_len, trg_len) #returns lower triangular matrix\n",
    "        return trg_mask\n",
    "    \n",
    "    def decode(self, src, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "        \n",
    "        out = trg\n",
    "        for i in range(seq_len):\n",
    "            out = self.decoder(out, enc_out, trg_mask)\n",
    "            \n",
    "            out = out[:, -1, :]\n",
    "            \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out, axis=0)\n",
    "            \n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src) \n",
    "        \n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15256046",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "decoder_input_size = 9\n",
    "target_output_size = 1\n",
    "num_layers = 6\n",
    "seq_length = 10 \n",
    "batch_size = 16\n",
    "dropout_value = 0.2\n",
    "num_training_stocks = 5\n",
    "\n",
    "\n",
    "model = Transformer(embed_dim=32, input_size=input_size, \n",
    "                    decoder_input_size=decoder_input_size, target_output_size=target_output_size, seq_length=seq_length,\n",
    "                    num_layers=num_layers, dropout_value=dropout_value, expansion_factor=4, n_heads=8) \n",
    "\n",
    "current_path = os.path.join('')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59feb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, inputs, labels, transform=None, target_transform=None):\n",
    "        self.labels = labels\n",
    "        self.inputs = inputs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inpt = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            inpt = self.transform(inpt)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inpt, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load same data as in Stocks Transformer\n",
    "mypath = os.path.join(current_path, '..', '..', 'Data', 'Network 1 data', 'Stocks')\n",
    "stocks = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "Xtr, Ytr = [], []\n",
    "Xval, Yval = [], []\n",
    "Xte, Yte = [], []\n",
    "\n",
    "block_size = input_size\n",
    "\n",
    "\n",
    "def build_dataset(prices):\n",
    "            X, Y = [], []\n",
    "            if len(prices)<block_size+1:\n",
    "                return [], []\n",
    "            for i in range(len(prices)-block_size):\n",
    "                X.append(prices[i:i+block_size])\n",
    "                Y.append(prices[i+block_size])\n",
    "            X = torch.tensor(X)\n",
    "            Y = torch.tensor(Y)\n",
    "            return X, Y\n",
    "\n",
    "for stock in stocks[1:num_training_stocks+1]:\n",
    "    data = open(os.path.join(mypath, stock)).read().splitlines()[1:]\n",
    "\n",
    "    year_lookup = {}\n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        year = int(lst[0][:4])\n",
    "        closing = float(lst[4])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [closing]\n",
    "        else:\n",
    "            year_lookup[year].append(closing)\n",
    "    \n",
    "    prices = []\n",
    "    train_prices = []\n",
    "    validation_prices = []\n",
    "    test_prices = []\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    years = list(year_lookup.keys())\n",
    "    if stock == stocks[num_training_stocks]:\n",
    "        for year in years:\n",
    "            test_prices += year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "    else:\n",
    "        training_cut_off = years[math.ceil(0.75*len(years))-1]\n",
    "        for year in range(years[0], training_cut_off):\n",
    "            train_prices+=year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "\n",
    "        for year in range(training_cut_off, years[-1]):\n",
    "            validation_prices+=year_lookup[year]\n",
    "            prices += year_lookup[year]\n",
    "\n",
    "    Xtr += build_dataset(train_prices)[0]\n",
    "    Ytr += build_dataset(train_prices)[1]\n",
    "    Xval += build_dataset(validation_prices)[0]\n",
    "    Yval += build_dataset(validation_prices)[1]\n",
    "    Xte, Yte = build_dataset(test_prices)\n",
    "training_data = CustomDataset(Xtr, Ytr)\n",
    "val_data = CustomDataset(Xval, Yval)\n",
    "test_data = CustomDataset(Xte, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d83ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best checkpoint using validation data\n",
    "best_checkpoint = 0\n",
    "losses = {}\n",
    "scores = {}\n",
    "for c in range(1, 151):\n",
    "    print(c)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        checkpoint = 10*c\n",
    "        state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"Without Smoothing\", '1checkpoint' + str(checkpoint) + '.pth'))\n",
    "        model.load_state_dict(state_dict)\n",
    "        inpt = [x for x in val_data[0][0]]\n",
    "        out = [x for x in val_data[0][0]]\n",
    "        for i in range(len(val_data)-10):\n",
    "            x = val_data[i][0]\n",
    "            y = val_data[i][1]\n",
    "            x = x.unsqueeze(0)\n",
    "            trg = x[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, trg)\n",
    "            out.append(output)\n",
    "            inpt.append(val_data[i+1][1])\n",
    "\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "    expected_loss = 0.0\n",
    "    for i in range(len(inpt)-5):\n",
    "        outpt = out[i+5].unsqueeze(0)\n",
    "        total_loss += (inpt[i+5]-out[i+5])**2\n",
    "        expected_loss += (inpt[i+5]-inpt[i+4])**2\n",
    "    losses[checkpoint] = total_loss/(len(inpt)-10)\n",
    "    \n",
    "    prod1 = 1\n",
    "    for i in range(1, len(inpt)):\n",
    "        sign = 1\n",
    "        if out[i]-out[i-1] < 0:\n",
    "            sign = -1\n",
    "        prod1 *= (1 + ((inpt[i]-inpt[i-1])/inpt[i-1]*sign))\n",
    "    scores[checkpoint] = prod1\n",
    "    \n",
    "sorted_losses = dict(sorted(losses.items(), key=lambda item: item[1]))\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1]), reverse=True)\n",
    "\n",
    "good_checkpoints = []\n",
    "threshold = min(losses.values()) + 2\n",
    "\n",
    "for check in losses.keys():\n",
    "    if losses[check] < threshold:\n",
    "        good_checkpoints.append(check)\n",
    "\n",
    "for check in sorted_scores.keys():\n",
    "    if check in good_checkpoints:\n",
    "        best_checkpoint = check\n",
    "        break\n",
    "\n",
    "print(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on aal.us\n",
    "state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"Without Smoothing\", '1checkpoint' + str(810) + '.pth'))\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inpt = [x for x in test_data[0][0]]\n",
    "    out = [x for x in test_data[0][0]]\n",
    "    for i in range(len(test_data)-10):\n",
    "        x = test_data[i][0]\n",
    "        y = test_data[i][1]\n",
    "        x = x.unsqueeze(0)\n",
    "        trg = x[:, 1:]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, trg)\n",
    "        out.append(output)\n",
    "        inpt.append(test_data[i+1][1])\n",
    "\n",
    "plt.plot(inpt, label='real data')\n",
    "plt.plot(out, alpha=0.5, label='predicted values')\n",
    "plt.xlabel('Days', fontsize=15)\n",
    "plt.ylabel('Closing Price', fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose 20 stocks with low volatility and similar price ranges\n",
    "good_stocks = []\n",
    "for stock in stocks:\n",
    "    data = open(os.path.join(mypath, stock)).read().splitlines()[1:]\n",
    "    year_lookup = {}\n",
    "    prices = []\n",
    "    \n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        year = int(lst[0][:4])\n",
    "        closing = float(lst[4])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [closing]\n",
    "        else:\n",
    "            year_lookup[year].append(closing)\n",
    "\n",
    "    years = list(year_lookup.keys())\n",
    "    for year in years[-5:]:\n",
    "        prices+=year_lookup[year]\n",
    "    \n",
    "    if len(prices) < 1000:\n",
    "        continue\n",
    "    \n",
    "    max1 = max(prices)\n",
    "    min1 = min(prices)\n",
    "    \n",
    "    grads = []\n",
    "    for i in range(10, len(prices)):\n",
    "        grads.append(abs(prices[i] - prices[i-10])/prices[i])\n",
    "    \n",
    "    maxgrad = max(grads)\n",
    "        \n",
    "    if maxgrad<0.17 and max1<100 and min1>20:\n",
    "        good_stocks.append(stock)\n",
    "\n",
    "print(len(good_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c986f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a set of test data from the above stocks\n",
    "test_datas = []\n",
    "for stock in good_stocks:\n",
    "    test_prices = []\n",
    "    data = open(os.path.join(mypath, stock)).read().splitlines()[1:]\n",
    "\n",
    "    year_lookup = {}\n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        year = int(lst[0][:4])\n",
    "        closing = float(lst[4])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [closing]\n",
    "        else:\n",
    "            year_lookup[year].append(closing)\n",
    "    \n",
    "    years = list(year_lookup.keys())\n",
    "    for year in years[-3:]:\n",
    "        test_prices += year_lookup[year]\n",
    "    Xte, Yte = build_dataset(test_prices)\n",
    "    test_datas.append(CustomDataset(Xte, Yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test network on the above test data\n",
    "checkpoint = 810\n",
    "state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"Without Smoothing\", '1checkpoint' + str(checkpoint) + '.pth'))\n",
    "model.load_state_dict(state_dict)\n",
    "prod2 = 1\n",
    "prods = []\n",
    "mses = []\n",
    "for s in range(len(test_datas)):\n",
    "    test_data = test_datas[s]\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        inpt = [x for x in test_data[0][0]]\n",
    "        out = [x for x in test_data[0][0]]\n",
    "        for i in range(len(test_data)-10):\n",
    "            x = test_data[i][0]\n",
    "            y = test_data[i][1]\n",
    "            x = x.unsqueeze(0)\n",
    "            trg = x[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, trg)\n",
    "            out.append(output)\n",
    "            inpt.append(test_data[i+1][1])\n",
    "\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(inpt)-10):\n",
    "        total_loss += (inpt[i+10]-out[i+10])**2\n",
    "    mse = total_loss/(len(inpt)-10)\n",
    "\n",
    "    prod1 = 1\n",
    "    for i in range(1, len(inpt)):\n",
    "        sign = 1\n",
    "        if out[i]-out[i-1] < 0:\n",
    "            sign = -1\n",
    "        prod1 *= (1+(inpt[i]-inpt[i-1])/inpt[i-1]*sign)\n",
    "        prod2 *= (1+(inpt[i]-inpt[i-1])/inpt[i-1]*sign)\n",
    "    score = prod1\n",
    "    \n",
    "    prods.append(score)\n",
    "    mses.append(mse)\n",
    "    print(checkpoint, mse, score)\n",
    "\n",
    "    plt.plot(inpt, label='real data')\n",
    "    plt.plot(out, alpha=0.5, label='predicted values')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print(prods, np.mean(prods), mses, np.mean(mses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
