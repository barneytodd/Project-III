{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "import torch.linalg as lg\n",
    "from scipy.interpolate import LSQUnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28be287",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformation(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer):\n",
    "        super(FeatureTransformation, self).__init__()\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(num_features, hidden_layer),\n",
    "                        nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    #embed inputs into a hidden_layer dimensional embedding space\n",
    "    def forward(self, z):\n",
    "        z = self.feed_forward(z)\n",
    "        return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgetGate(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(ForgetGate, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    #runs the forget gate of an LSTM network   \n",
    "    def forward(self, h_tminus1, z_t):\n",
    "        zt = self.linear1(z_t)\n",
    "        htminus1 = self.linear2(h_tminus1)\n",
    "        sum_zh = (zt + htminus1)\n",
    "        ft = self.sigmoid(sum_zh)\n",
    "        return ft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46757e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGate(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(InputGate, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear3 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear4 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    #runs the input gate of an LSTM network  \n",
    "    def forward(self, h_tminus1, z_t):\n",
    "        zt_1 = self.linear1(z_t)\n",
    "        htminus1_1 = self.linear2(h_tminus1)\n",
    "        sum_zh_1 = zt_1 + htminus1_1\n",
    "        it1 = self.sigmoid(sum_zh_1)\n",
    "\n",
    "        zt_2 = self.linear3(z_t)\n",
    "        htminus1_2 = self.linear4(h_tminus1)\n",
    "        sum_zh_2 = zt_2 + htminus1_2\n",
    "        it2 = self.tanh(sum_zh_2)\n",
    "\n",
    "        out = it1*it2\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGate(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(OutputGate, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    #runs the output gate of an LSTM network  \n",
    "    def forward(self, h_tminus1, z_t):\n",
    "        zt = self.linear1(z_t)\n",
    "        htminus1 = self.linear2(h_tminus1)\n",
    "        sum_zh = zt + htminus1\n",
    "        ot = self.sigmoid(sum_zh)\n",
    "        return ot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        \n",
    "        self.forget_gate = ForgetGate(hidden_layer)\n",
    "        self.input_gate = InputGate(hidden_layer)\n",
    "        self.output_gate = OutputGate(hidden_layer)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    #runs 1 LSTM cell\n",
    "    def forward(self, h_tminus1, c_tminus1, z_t):\n",
    "        ft = self.forget_gate(h_tminus1, z_t)\n",
    "        it = self.input_gate(h_tminus1, z_t)\n",
    "        ot = self.output_gate(h_tminus1, z_t)\n",
    "        c_t = ft*c_tminus1 + it\n",
    "        tanc_t = self.tanh(c_t)\n",
    "        h_t = ot * tanc_t\n",
    "        return h_t, c_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.feature_transformation = FeatureTransformation(num_features, hidden_layer)\n",
    "        self.lstm_cell = LSTMCell(hidden_layer)\n",
    "        self.hidden_layer = hidden_layer\n",
    "    \n",
    "    #runs the feature embedding and then runs the complete LSTM network\n",
    "    #outputs a tensor where each row is the short term memory output of the corresponding LSTM cell\n",
    "    def forward(self, z):\n",
    "        z_t = self.feature_transformation(z) \n",
    "        batch_size = z_t.size(0)\n",
    "        num_stocks = z_t.size(1)\n",
    "        h = torch.zeros(batch_size, num_stocks, self.hidden_layer)\n",
    "        c = torch.zeros(batch_size, num_stocks, self.hidden_layer)\n",
    "        T = z.size(2)\n",
    "        h_outputs = []\n",
    "        for t in range(T):\n",
    "            h_t, c_t = self.lstm_cell(h, c, z_t[:, :, t])\n",
    "            h_outputs.append(h_t)\n",
    "            h = h_t.clone().detach()  \n",
    "            c = c_t.clone().detach()  \n",
    "        return torch.stack(h_outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer, input_size):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = LSTM(num_features, hidden_layer)\n",
    "        self.W = torch.nn.Parameter\n",
    "        self.W.requires_grad = True\n",
    "        self.b = torch.nn.Parameter\n",
    "        self.b.requires_grad = True\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer = hidden_layer\n",
    "    \n",
    "    #applies context normalisation as defined in the original paper\n",
    "    def context_norm(self, h_c, batch_size, num_stocks):\n",
    "        mean = torch.mean(h_c)\n",
    "        std = torch.std(h_c)\n",
    "        hc_adj = (h_c-mean)/std\n",
    "        W = self.W(torch.randn((batch_size, num_stocks, self.hidden_layer)))\n",
    "        b = self.b(torch.randn((batch_size, num_stocks, self.hidden_layer)))\n",
    "        hc_adj *= W\n",
    "        hc_adj += b\n",
    "        return hc_adj\n",
    "    \n",
    "    #applies the attention weighting to the LSTM outputs\n",
    "    def forward(self, z):\n",
    "        h = self.lstm(z) \n",
    "        batch_size = z.size(0)\n",
    "        num_stocks = z.size(1)\n",
    "        sum_exp = torch.zeros(batch_size, num_stocks)\n",
    "        for i in range(len(h)):\n",
    "            sum_exp1 = sum_exp + torch.exp(torch.sum(h[i]*h[-1], 2))\n",
    "            sum_exp = sum_exp1\n",
    "        alpha = torch.zeros(self.input_size, batch_size, num_stocks) \n",
    "        for i in range(len(h)):\n",
    "            alpha_i = alpha[i] + torch.exp(torch.sum(h[i]*h[-1], 2))/sum_exp\n",
    "            alpha[i] = alpha_i\n",
    "        alpha = alpha.unsqueeze(3) \n",
    "        \n",
    "        h_alpha = h * alpha \n",
    "        h_c = torch.zeros((batch_size, num_stocks, self.hidden_layer))\n",
    "        for i in range(len(h)):\n",
    "            h_c2 = h_c + h_alpha[i]\n",
    "            h_c = h_c2\n",
    "        \n",
    "        h_c1 = self.context_norm(h_c, batch_size, num_stocks)\n",
    "        \n",
    "        return h_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e193703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLContext(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer, input_size, beta):\n",
    "        super(MLContext, self).__init__()\n",
    "        \n",
    "        self.attention_lstm = AttentionLSTM(num_features, hidden_layer, input_size)\n",
    "        self.beta = beta\n",
    "    \n",
    "    #adds context from the market index data\n",
    "    def forward(self, z, market_index):\n",
    "        h_c = self.attention_lstm(z)\n",
    "        h_i = self.attention_lstm(market_index)\n",
    "        h_m = h_c + self.beta*h_i\n",
    "        return h_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer, input_size, beta, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.ml_context = MLContext(num_features, hidden_layer, input_size, beta)\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.n_heads = n_heads\n",
    "        self.single_head_dim = int(self.hidden_layer / self.n_heads) \n",
    "        \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False) \n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False) \n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False) \n",
    "        \n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.hidden_layer) \n",
    "    \n",
    "    #applies multi-head attention between the different context vectors\n",
    "    def forward(self, z, market_index):\n",
    "        h_m = self.ml_context(z, market_index)\n",
    "        key, query, value = h_m, h_m, h_m\n",
    "        batch_size = key.size(0)\n",
    "        num_stocks = key.size(1)\n",
    "    \n",
    "        key = key.view(batch_size, num_stocks, self.n_heads, self.single_head_dim) \n",
    "        query = query.view(batch_size, num_stocks, self.n_heads, self.single_head_dim) \n",
    "        value = value.view(batch_size, num_stocks, self.n_heads, self.single_head_dim) \n",
    "\n",
    "        k = self.key_matrix(key).transpose(1, 2) \n",
    "        q = self.query_matrix(query).transpose(1, 2)   \n",
    "        v = self.value_matrix(value).transpose(1, 2) \n",
    "\n",
    "        k_adjusted = k.transpose(-1, -2)\n",
    "\n",
    "        product = torch.matmul(q, k_adjusted)\n",
    "\n",
    "        product = product / math.sqrt(self.single_head_dim)\n",
    "\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "        scores2 = torch.matmul(scores, v) \n",
    "        concat = scores2.transpose(1,2).contiguous().view(batch_size, num_stocks, self.single_head_dim*self.n_heads) #32 x 10 x 512 #seq_length, \n",
    "\n",
    "        output = self.out(concat)\n",
    "        return h_m, output, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalOutput(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer, input_size, beta, n_heads, expansion_factor=4, dropout_value=0.2):\n",
    "        super(FinalOutput, self).__init__()\n",
    "        \n",
    "        #self.ml_context = MLContext(num_features, hidden_layer, input_size, beta)\n",
    "        self.multihead = MultiHeadAttention(num_features, hidden_layer, input_size, beta, n_heads)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(hidden_layer, expansion_factor*hidden_layer),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(expansion_factor*hidden_layer, hidden_layer)\n",
    "        )\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = nn.Linear(hidden_layer, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_layer)\n",
    "        self.norm2 = nn.LayerNorm(hidden_layer)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_value)\n",
    "        self.dropout2 = nn.Dropout(dropout_value)\n",
    "    \n",
    "    #produces the final output, along with the attention map\n",
    "    def forward(self, z, market_index):\n",
    "        h_m1, h_tilda, scores = self.multihead(z, market_index)\n",
    "        h_tilda_residual = h_tilda + h_m1\n",
    "        h_attention_out = self.dropout1(self.norm1(h_tilda_residual))\n",
    "        h_linear = self.feed_forward(h_attention_out)\n",
    "        h_linear_residual = self.tanh(h_linear + h_attention_out)\n",
    "        h_p = self.dropout2(self.norm2(h_linear_residual))\n",
    "        h_p2 = self.linear(h_p)\n",
    "        y = self.sigmoid(h_p2).squeeze()\n",
    "        return y, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900c8b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initiates the model with all of its required parameters \n",
    "num_features = 11\n",
    "hidden_layer = 128\n",
    "beta = 0.1\n",
    "input_size = 10\n",
    "n_heads = 8\n",
    "expansion_factor = 4\n",
    "dropout_value = 0.2\n",
    "num_training_stocks = 5\n",
    "batch_size = 16\n",
    "\n",
    "current_path = os.path.join('')\n",
    "checkpoints_path = os.path.join(current_path, \"Checkpoints\")\n",
    "\n",
    "model = FinalOutput(num_features, hidden_layer, input_size, beta, n_heads, expansion_factor, dropout_value) #, beta, n_heads, expansion_factor, dropout_value)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dataset of inputs paired with labels\n",
    "class CustomDataset():\n",
    "    def __init__(self, inputs, labels, transform=None, target_transform=None):\n",
    "        self.labels = labels\n",
    "        self.inputs = inputs\n",
    "        #self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        inpt = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            inpt = self.transform(inpt)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inpt, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the market index data and produces a list of features\n",
    "MI_path = os.path.join(current_path, \"..\", \"..\", \"Data\", \"Network 2 data\", \"Market Index data\")\n",
    "years = [f for f in listdir(MI_path) if isfile(join(MI_path, f))]\n",
    "MI_data = []\n",
    "for year in years:\n",
    "    data2 = list(reversed(open(os.path.join(MI_path, year)).read().splitlines()[1:]))\n",
    "    for i in range(len(data2)):\n",
    "        lst = data2[i].split(',\"')\n",
    "        for j in range(len(lst)-1):\n",
    "            lst[j+1] = lst[j+1][:-1]\n",
    "            if ',' in lst[j+1]:\n",
    "                idx = lst[j+1].find(',')\n",
    "                lst[j+1] = lst[j+1][:idx] + lst[j+1][idx+1:]\n",
    "        lst += ['volume', lst[-1]]\n",
    "        data2[i] = ','.join(lst)\n",
    "    MI_data+=data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b326df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the dataset and creates a list of features for each stock\n",
    "mypath = os.path.join(current_path, \"..\", \"..\", \"Data\", \"Network 2 data\", \"Adv-ALSTM-master\", \"data\", \"kdd17\", \"price_long_50\")\n",
    "stocks = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "\n",
    "good_stocks = []\n",
    "for stock in stocks:\n",
    "    data = open(os.path.join(mypath, stock)).read().splitlines()\n",
    "    data = list(reversed(data[1:]))\n",
    "    date1 = data[0].split(',')[0].replace('-', '/').split('/')\n",
    "    date2 = data[1].split(',')[0].replace('-', '/').split('/')\n",
    "    min_year = 10000\n",
    "    max_year = 0\n",
    "    year = 0\n",
    "    for i in range(len(date1)):\n",
    "        if len(date1[i]) == 4:\n",
    "            year = i\n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        date_lst = lst[0].replace('-', '/').split('/')   #.split('/')\n",
    "        year1 = int(date_lst[year])\n",
    "        if year1 > max_year:\n",
    "            max_year = year1\n",
    "        if year1 < min_year:\n",
    "            min_year = year1\n",
    "    if min_year <= 2010 and max_year >= 2016 and len(data)>1500:\n",
    "        good_stocks.append(stock)\n",
    "stocks = good_stocks\n",
    "\n",
    "print(len(stocks))\n",
    "    \n",
    "feature_vectors = {}\n",
    "for i in range(num_training_stocks+1):\n",
    "    if i == 0:\n",
    "        data = MI_data\n",
    "    else:\n",
    "        data = open(os.path.join(mypath, stocks[i-1])).read().splitlines()\n",
    "        data = list(reversed(data[1:]))\n",
    "    date1 = data[0].split(',')[0].replace('-', '/').split('/')\n",
    "    date2 = data[1].split(',')[0].replace('-', '/').split('/')\n",
    "    year = 0\n",
    "    month = 0\n",
    "    day_of_month = 0\n",
    "    for i in range(len(date1)):\n",
    "        if len(date1[i]) == 4:\n",
    "            year = i\n",
    "        elif date1[i] == date2[i]:\n",
    "            month = i\n",
    "        else:\n",
    "            day_of_month = i\n",
    "    \n",
    "    previous_adjclose = []\n",
    "    previous_close = 0\n",
    "    previous_date = []\n",
    "    \n",
    "    for day in data[:30]: # build list of 30 days of adj_close\n",
    "        lst = day.split(',')\n",
    "        adj_close = float(lst[6])\n",
    "        previous_adjclose.append(adj_close)\n",
    "        if day == data[29]:\n",
    "            previous_close = float(lst[4]) \n",
    "    for day in data[31:]:    # add feature vector to feature_vectors[date]\n",
    "        lst = day.split(',')\n",
    "        date_lst = lst[0].replace('-', '/').split('/')   #.split('/')\n",
    "        date = '/'.join([str(int(date_lst[day_of_month])), str(int(date_lst[month])), str(int(date_lst[year]))])\n",
    "       \n",
    "        open_price = float(lst[1])\n",
    "        close_price = float(lst[4])\n",
    "        high = float(lst[2])\n",
    "        low = float(lst[3])\n",
    "        adj_close = float(lst[6])\n",
    "        feature_vector = [(open_price/close_price)-1, (high/close_price)-1, (low/close_price)-1, (close_price/previous_close)-1, (adj_close/previous_adjclose[-1])-1]\n",
    "        for i in range(5, 35, 5):\n",
    "            feature_vector.append((sum(previous_adjclose[-i:])/(i*adj_close))-1)\n",
    "        if date not in feature_vectors.keys():\n",
    "            feature_vectors[date] = [feature_vector]\n",
    "        else:\n",
    "            feature_vectors[date].append(feature_vector)\n",
    "        previous_close = close_price\n",
    "        previous_adjclose.append(adj_close)\n",
    "\n",
    "#separates the years within the data\n",
    "year_lookup = {} \n",
    "for date in feature_vectors.keys(): \n",
    "    if len(feature_vectors[date]) != num_training_stocks+1:\n",
    "        continue\n",
    "    else:\n",
    "        date_lst = date.split('/')\n",
    "        year = int(date_lst[2])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [feature_vectors[date]]\n",
    "        else:\n",
    "            year_lookup[year].append(feature_vectors[date])\n",
    "\n",
    "train_feature_vectors = []\n",
    "val_feature_vectors = []\n",
    "test_feature_vectors = []\n",
    "years = sorted(list(year_lookup.keys()))\n",
    "test_feature_vectors = []\n",
    "#separates the data into training, validation and test datasets\n",
    "\n",
    "print(years)\n",
    "test_feature_vectors += year_lookup[years[-1]]\n",
    "\n",
    "training_cut_off = math.ceil(0.75*len(years))-1\n",
    "for year in range(training_cut_off):\n",
    "    train_feature_vectors += year_lookup[years[year]]\n",
    "for year in range(training_cut_off, len(years)):\n",
    "    val_feature_vectors += year_lookup[years[year]]\n",
    "\n",
    "\n",
    "\n",
    "block_size = input_size\n",
    "\n",
    "#builds dataset of 10 consecutive days for input paired with the next day for label\n",
    "def build_dataset(feature_vectors):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(feature_vectors)-block_size):\n",
    "        X.append(feature_vectors[i:i+block_size])\n",
    "        Y_builder = []\n",
    "        for stock in range(len(feature_vectors[0])):\n",
    "            if feature_vectors[i+block_size][stock][3]>0:\n",
    "                Y_builder.append(1)\n",
    "            else:\n",
    "                Y_builder.append(0)\n",
    "        Y.append(Y_builder)\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "#builds the datasets so that they are ready to be inputted into the model\n",
    "Xtr, Ytr = build_dataset(train_feature_vectors)\n",
    "Xval, Yval = build_dataset(val_feature_vectors)\n",
    "Xte, Yte = build_dataset(test_feature_vectors)\n",
    "\n",
    "training_data = CustomDataset(Xtr, Ytr)\n",
    "val_data = CustomDataset(Xval, Yval)\n",
    "test_data = CustomDataset(Xval, Yval)\n",
    "\n",
    "trainloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains the model\n",
    "epochs = 1500\n",
    "checkpoint=0\n",
    "train_losses, val_losses = [], []\n",
    "for e in range(checkpoint+1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    tot_train_loss = 0\n",
    "    model.train()\n",
    "    for x, y in trainloader:\n",
    "        if len(y) != batch_size:\n",
    "            continue\n",
    "        z = torch.transpose(x, 1, 2)\n",
    "        market_index = z[:, 0]\n",
    "        market_index = torch.unsqueeze(market_index, 1)\n",
    "        z = z[:, 1:]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(z, market_index)\n",
    "        y = y[:, 1:]\n",
    "        loss = criterion(outputs[0].float(), y.float()) + abs(model.linear.bias) + lg.matrix_norm(model.linear.weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_train_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        tot_val_loss = 0        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, y in valloader:\n",
    "                if len(y) != batch_size:\n",
    "                    continue\n",
    "                z = torch.transpose(x, 1, 2)\n",
    "                market_index = z[:, 0]\n",
    "                market_index = torch.unsqueeze(market_index, 1)\n",
    "                z = z[:, 1:]\n",
    "                outputs = model(z, market_index)\n",
    "                y = y[:, 1:]\n",
    "                \n",
    "                loss = criterion(outputs[0].float(), y.float()) + abs(list(model.parameters())[-5]) + lg.matrix_norm(list(model.parameters())[-6])\n",
    "                tot_val_loss += loss.item()#\n",
    "\n",
    "\n",
    "        # Get mean loss to enable comparison between train and test sets\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        val_loss = tot_val_loss / len(valloader.dataset)\n",
    "\n",
    "        # At completion of epoch print losses to follow progress\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.5f}.. \".format(train_loss),\n",
    "              \"Test Loss: {:.5f}.. \".format(val_loss), \n",
    "              \"Time taken: {:.3f}..\".format(time.time()-start_time))\n",
    "    #after every 10 epochs, save the model parameters to avoiding losing them in the case that we have to reload the program\n",
    "    if e%10 == 0 and e!=0:\n",
    "        torch.save(model.state_dict(), os.join(current_path, \"Checkpoints\", '2checkpoint' + str(e) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model to find the best checkpoints using validation dataset\n",
    "percentages = []\n",
    "best = [0,0]\n",
    "for c in range(1,151):\n",
    "    print(c)\n",
    "    checkpoint = c*10\n",
    "    state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"2checkpoint\" + str(checkpoint) + \".pth\"))\n",
    "    model.load_state_dict(state_dict)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        inpt = []\n",
    "        out = []\n",
    "        for i in range(len(val_data)):\n",
    "            x = val_data.inputs[i].unsqueeze(0)\n",
    "            y = val_data.labels[i].unsqueeze(0)\n",
    "            z = torch.transpose(x, 1, 2)\n",
    "            market_index = z[:,0,:]\n",
    "            market_index = torch.unsqueeze(market_index, 1)\n",
    "            z = z[:, 1:]\n",
    "            outputs = model(z, market_index)[0]\n",
    "            y = y[:, 1:]\n",
    "            out.append(outputs)\n",
    "            inpt.append(y.squeeze())\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(inpt)):\n",
    "        for j in range(len(inpt[0])):\n",
    "            total+=1\n",
    "            if (inpt[i][j]==1 and out[i][j] > 0.5) or (inpt[i][j]==0 and out[i][j]<0.5):\n",
    "                correct+=1\n",
    "    percentages.append(correct/total)  \n",
    "    if correct/total > best[1]:\n",
    "        best = [c,correct/total]\n",
    "\n",
    "print(percentages)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model using best checkpoint to test its effectiveness\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "checkpoint = 1020\n",
    "state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"2checkpoint\" + str(checkpoint) + \".pth\"))\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inpt = []\n",
    "    out = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = test_data.inputs[i].unsqueeze(0)\n",
    "        y = test_data.labels[i].unsqueeze(0)\n",
    "        z = torch.transpose(x, 1, 2)\n",
    "        market_index = z[:,0,:]\n",
    "        market_index = torch.unsqueeze(market_index, 1)\n",
    "        z = z[:, 1:]\n",
    "        outputs = model(z, market_index)[0]\n",
    "        y = y[:, 1:]\n",
    "        out.append(outputs)\n",
    "        inpt.append(y.squeeze())\n",
    "\n",
    "proportion = {0:[0,0], 1:[0,0], 2:[0,0]}\n",
    "correct = 0\n",
    "total = 0\n",
    "prod1 = [1,1,1,1,1]\n",
    "for i in range(len(inpt)-1):\n",
    "    for j in range(len(inpt[0])):\n",
    "        total+=1\n",
    "        if (inpt[i][j]==1 and out[i][j] > 0.5) or (inpt[i][j]==0 and out[i][j]<0.5):\n",
    "            correct+=1\n",
    "            proportion[int(inpt[i][j])][0] += 1\n",
    "        elif out[i][j] != 0.5:\n",
    "            proportion[int(inpt[i][j])][1] += 1\n",
    "        else:\n",
    "            proportion[2][inpt[i][j]] += 1\n",
    "        \n",
    "        sign = 1\n",
    "        if out[i][j] < 0.5:\n",
    "            sign = -1\n",
    "        elif out[i][j] == 0.5:\n",
    "            sign = 0\n",
    "        prod1[j] *= (1+test_data.inputs[i+1, 9, j, 3]*sign)\n",
    "print(correct/total)\n",
    "print(proportion)\n",
    "print(prod1)\n",
    "print(np.mean(prod1))\n",
    "print(min(prod1))\n",
    "print(max(prod1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the attention scores and normalised+smoothed attention scores between the first two stocks\n",
    "#compare to naive method for calculating stock correlations\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "checkpoint = 1020\n",
    "state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"2checkpoint\" + str(checkpoint) + \".pth\"))\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inpt = []\n",
    "    out = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = test_data.inputs[i].unsqueeze(0)\n",
    "        y = test_data.labels[i].unsqueeze(0)\n",
    "        z = torch.transpose(x, 1, 2)\n",
    "        market_index = z[:,0,:]\n",
    "        market_index = torch.unsqueeze(market_index, 1)\n",
    "        z = z[:, 1:]\n",
    "        outputs = torch.mean(model(z, market_index)[1].squeeze(), dim=0)\n",
    "        y = y[:, 1:]\n",
    "        out.append(outputs)\n",
    "        inpt.append(y.squeeze())\n",
    "\n",
    "means = []\n",
    "for i in range(len(out[0])):\n",
    "    mean = np.mean([out[day][i][i] for day in range(len(out))])\n",
    "    means.append(mean)   \n",
    "    \n",
    "print(means)\n",
    "\n",
    "onetotwo = [out[day][0][1] for day in range(len(out))]\n",
    "twotoone = [out[day][1][0] for day in range(len(out))]\n",
    "\n",
    "plt.plot(twotoone, color = 'orange', label = 'Dependency of stock 2 on stock 1')\n",
    "plt.plot(onetotwo, color='blue', alpha=0.5, label = 'Dependency of stock 1 on stock 2')\n",
    "plt.xlabel(\"Days\", fontsize=15)\n",
    "plt.ylabel(\"Correlation Coefficient\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "num_datapoints = 20\n",
    "num_knots = max(0, math.floor(len(onetotwo)/num_datapoints-1))\n",
    "time = np.arange(len(onetotwo))\n",
    "\n",
    "knot_indices = np.linspace(num_datapoints, len(time) - 1-num_datapoints, num_knots).astype(int)\n",
    "knots = time[knot_indices]\n",
    "\n",
    "cs1 = LSQUnivariateSpline(time, onetotwo, k=3, t=knots)\n",
    "smooth_prices1 = cs1(time)\n",
    "\n",
    "cs2 = LSQUnivariateSpline(time, twotoone, k=3, t=knots)\n",
    "smooth_prices2 = cs2(time)\n",
    "\n",
    "smooth_prices1 = (smooth_prices1 - np.mean(smooth_prices1))/np.std(smooth_prices1)\n",
    "smooth_prices2 = (smooth_prices2 - np.mean(smooth_prices2))/np.std(smooth_prices2)\n",
    "\n",
    "\n",
    "plt.plot(smooth_prices1, alpha=0.5, label = 'Dependency of stock 1 on stock 2')\n",
    "plt.plot(smooth_prices2, label = 'Dependency of stock 2 on stock 1')\n",
    "plt.xlabel(\"Days\", fontsize=15)\n",
    "plt.ylabel(\"Normalised Correlation Coefficient\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "stock1prices = test_data.inputs[:,:,1,3].squeeze()\n",
    "stock2prices = test_data.inputs[:,:,2,3].squeeze()\n",
    "\n",
    "correl = []\n",
    "for day in range(len(stock1prices)):\n",
    "    sum1 = 0\n",
    "    for i in range(len(stock1prices[day])):\n",
    "        if stock1prices[day][i] * stock2prices[day][i] > 0:\n",
    "            sum1+=1\n",
    "        else:\n",
    "            sum1-=1\n",
    "    correl.append(sum1)\n",
    "    \n",
    "correl = (correl - np.mean(correl))/np.std(correl)\n",
    "\n",
    "num_datapoints = 20\n",
    "num_knots = max(0, math.floor(len(correl)/num_datapoints-1))\n",
    "time = np.arange(len(onetotwo))\n",
    "\n",
    "knot_indices = np.linspace(num_datapoints, len(time) - 1-num_datapoints, num_knots).astype(int)\n",
    "knots = time[knot_indices]\n",
    "\n",
    "cs3 = LSQUnivariateSpline(time, correl, k=3, t=knots)\n",
    "correl = cs3(time)\n",
    "\n",
    "plt.plot(smooth_prices1, alpha = 0.5, label = 'Dependency of stock 1 on stock 2')\n",
    "plt.plot(correl, color='green', label = \"Naive correlation between stock 1 and 2\")\n",
    "plt.xlabel(\"Days\", fontsize=15)\n",
    "plt.ylabel(\"Normalised Correlation Coefficient\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(smooth_prices2, color='orange', alpha = 0.5, label = 'Dependency of stock 2 on stock 1')\n",
    "plt.plot(correl, color='green', label = \"Naive correlation between stock 1 and 2\")\n",
    "plt.xlabel(\"Days\", fontsize=15)\n",
    "plt.ylabel(\"Normalised Correlation Coefficient\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b949809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the attention scores of each stock with itself\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "checkpoint = 1020\n",
    "state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", \"2checkpoint\" + str(checkpoint) + \".pth\"))\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inpt = []\n",
    "    out = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = test_data.inputs[i].unsqueeze(0)\n",
    "        y = test_data.labels[i].unsqueeze(0)\n",
    "        z = torch.transpose(x, 1, 2)\n",
    "        market_index = z[:,0,:]\n",
    "        market_index = torch.unsqueeze(market_index, 1)\n",
    "        z = z[:, 1:]\n",
    "        outputs = torch.mean(model(z, market_index)[1].squeeze(), dim=0)\n",
    "        y = y[:, 1:]\n",
    "        out.append(outputs)\n",
    "        inpt.append(y.squeeze())\n",
    "\n",
    "means = []\n",
    "for i in range(len(out[0])):\n",
    "    mean = np.mean([out[day][i][i] for day in range(len(out))])\n",
    "    means.append(mean)   \n",
    "    \n",
    "print(means)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
