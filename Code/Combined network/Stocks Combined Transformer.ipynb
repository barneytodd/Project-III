{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6af7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "from scipy.interpolate import LSQUnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8218a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this network we combine Transformer 1 and Transformer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7826cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(1, embed_dim)\n",
    "    \n",
    "    #embed the inputs into an embed_dim dimensional embedding space using a linear layer\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        out = self.linear(x)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_model_dim\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, self.embed_dim) \n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/self.embed_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        \n",
    "        pe = pe.unsqueeze(0) \n",
    "        self.register_buffer('pe', pe) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x *= math.sqrt(self.embed_dim) \n",
    "        seq_len = x.size(2)\n",
    "        x += torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformation(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer):\n",
    "        super(FeatureTransformation, self).__init__()\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(num_features, hidden_layer),\n",
    "                        nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = self.feed_forward(z)\n",
    "        return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgetGate(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(ForgetGate, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, h_tminus1, z_t):\n",
    "        zt = self.linear1(z_t)\n",
    "        htminus1 = self.linear2(h_tminus1)\n",
    "        sum_zh = (zt + htminus1)\n",
    "        ft = self.sigmoid(sum_zh)\n",
    "        return ft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGate(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(InputGate, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear3 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear4 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, h_tminus1, z_t):\n",
    "        zt_1 = self.linear1(z_t)\n",
    "        htminus1_1 = self.linear2(h_tminus1)\n",
    "        sum_zh_1 = zt_1 + htminus1_1\n",
    "        it1 = self.sigmoid(sum_zh_1)\n",
    "\n",
    "        zt_2 = self.linear3(z_t)\n",
    "        htminus1_2 = self.linear4(h_tminus1)\n",
    "        sum_zh_2 = zt_2 + htminus1_2\n",
    "        it2 = self.tanh(sum_zh_2)\n",
    "\n",
    "        out = it1*it2\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e10dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGate(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(OutputGate, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, h_tminus1, z_t):\n",
    "        zt = self.linear1(z_t)\n",
    "        htminus1 = self.linear2(h_tminus1)\n",
    "        sum_zh = zt + htminus1\n",
    "        ot = self.sigmoid(sum_zh)\n",
    "        return ot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a08849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        \n",
    "        self.forget_gate = ForgetGate(hidden_layer)\n",
    "        self.input_gate = InputGate(hidden_layer)\n",
    "        self.output_gate = OutputGate(hidden_layer)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, h_tminus1, c_tminus1, z_t):\n",
    "        ft = self.forget_gate(h_tminus1, z_t)\n",
    "        it = self.input_gate(h_tminus1, z_t)\n",
    "        ot = self.output_gate(h_tminus1, z_t)\n",
    "        c_t = ft*c_tminus1 + it\n",
    "        tanc_t = self.tanh(c_t)\n",
    "        h_t = ot * tanc_t\n",
    "        return h_t, c_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.feature_transformation = FeatureTransformation(num_features, hidden_layer)\n",
    "        self.lstm_cell = LSTMCell(hidden_layer)\n",
    "        self.hidden_layer = hidden_layer\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z_t = self.feature_transformation(z) \n",
    "        batch_size = z_t.size(0)\n",
    "        num_stocks = z_t.size(1)\n",
    "        h = torch.zeros(batch_size, num_stocks, self.hidden_layer)\n",
    "        c = torch.zeros(batch_size, num_stocks, self.hidden_layer)\n",
    "        T = z.size(2)\n",
    "        h_outputs = []\n",
    "        for t in range(T):\n",
    "            h_t, c_t = self.lstm_cell(h, c, z_t[:, :, t])\n",
    "            h_outputs.append(h_t)\n",
    "            h = h_t.clone().detach()  \n",
    "            c = c_t.clone().detach()  \n",
    "        return torch.stack(h_outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module): \n",
    "    def __init__(self, num_features, hidden_layer, input_size):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = LSTM(num_features, hidden_layer)\n",
    "        self.W = torch.nn.Parameter\n",
    "        self.W.requires_grad = True\n",
    "        self.b = torch.nn.Parameter\n",
    "        self.b.requires_grad = True\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer = hidden_layer\n",
    "    \n",
    "    def context_norm(self, h_c, batch_size, num_stocks): \n",
    "        mean = torch.mean(h_c)\n",
    "        std = torch.std(h_c)\n",
    "        hc_adj = (h_c-mean)/std\n",
    "        W = self.W(torch.randn((batch_size, num_stocks, self.hidden_layer)))\n",
    "        b = self.b(torch.randn((batch_size, num_stocks, self.hidden_layer)))\n",
    "        hc_adj *= W\n",
    "        hc_adj += b\n",
    "        return hc_adj\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self.lstm(z) \n",
    "        batch_size = z.size(0)\n",
    "        num_stocks = z.size(1)\n",
    "        sum_exp = torch.zeros(batch_size, num_stocks)\n",
    "        for i in range(len(h)):\n",
    "            sum_exp1 = sum_exp + torch.exp(torch.sum(h[i]*h[-1], 2))\n",
    "            sum_exp = sum_exp1\n",
    "        alpha = torch.zeros(self.input_size, batch_size, num_stocks) \n",
    "        for i in range(len(h)):\n",
    "            alpha_i = alpha[i] + torch.exp(torch.sum(h[i]*h[-1], 2))/sum_exp\n",
    "            alpha[i] = alpha_i\n",
    "        alpha = alpha.unsqueeze(3) \n",
    "        \n",
    "        h_alpha = h * alpha \n",
    "        h_c = torch.zeros((batch_size, num_stocks, self.hidden_layer))\n",
    "        for i in range(len(h)):\n",
    "            h_c2 = h_c + h_alpha[i]\n",
    "            h_c = h_c2\n",
    "        \n",
    "        h_c1 = self.context_norm(h_c, batch_size, num_stocks)\n",
    "        return h_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11492d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLContext(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer, input_size, beta):\n",
    "        super(MLContext, self).__init__()\n",
    "        \n",
    "        self.attention_lstm = AttentionLSTM(num_features, hidden_layer, input_size)\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, z, market_index):\n",
    "        h_c = self.attention_lstm(z)\n",
    "        h_i = self.attention_lstm(market_index)\n",
    "        h_m = h_c + self.beta*h_i\n",
    "        return h_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d72db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads) \n",
    "        \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False) \n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False) \n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False) \n",
    "        \n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "    \n",
    "    #this section is modified to accomodate the use in both networks\n",
    "    def forward(self, key, query, value, mask=None, trans=True):\n",
    "        batch_size = key.size(0)\n",
    "        num_stocks = key.size(1)\n",
    "        seq_length = key.size(2)\n",
    "        \n",
    "        seq_length_query = query.size(2)\n",
    "        \n",
    "        if trans:\n",
    "            key = key.view(batch_size, num_stocks, seq_length, self.n_heads, self.single_head_dim) \n",
    "            query = query.view(batch_size, num_stocks, seq_length_query, self.n_heads, self.single_head_dim)\n",
    "            value = value.view(batch_size, num_stocks, seq_length, self.n_heads, self.single_head_dim)\n",
    "\n",
    "            k = self.key_matrix(key).transpose(3, 2) \n",
    "            q = self.query_matrix(query).transpose(3, 2)   \n",
    "            v = self.value_matrix(value).transpose(3, 2) \n",
    "        \n",
    "        else:\n",
    "            key = key.view(batch_size, num_stocks, self.n_heads, self.single_head_dim) \n",
    "            query = query.view(batch_size, num_stocks, self.n_heads, self.single_head_dim)\n",
    "            value = value.view(batch_size, num_stocks, self.n_heads, self.single_head_dim)\n",
    "\n",
    "            k = self.key_matrix(key).transpose(1, 2) \n",
    "            q = self.query_matrix(query).transpose(1, 2)   \n",
    "            v = self.value_matrix(value).transpose(1, 2) \n",
    "\n",
    "        k_adjusted = k.transpose(-1, -2) \n",
    "\n",
    "        product = torch.matmul(q, k_adjusted)\n",
    "\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "\n",
    "        product = product / math.sqrt(self.single_head_dim) \n",
    "\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "        scores1 = torch.matmul(scores, v) \n",
    "        if trans:\n",
    "            concat = scores1.transpose(1,2).contiguous().view(batch_size, num_stocks, seq_length_query, self.single_head_dim*self.n_heads) \n",
    "        \n",
    "        else:\n",
    "            concat = scores1.transpose(1,2).contiguous().view(batch_size, num_stocks, self.single_head_dim*self.n_heads) \n",
    "\n",
    "        \n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92cdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the same as FinalOutput from network 2\n",
    "class CorrelationsOutput(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer, input_size, beta, n_heads, expansion_factor=4, dropout_value=0.2):\n",
    "        super(CorrelationsOutput, self).__init__()\n",
    "        \n",
    "        self.mlcontext = MLContext(num_features, hidden_layer, input_size, beta)\n",
    "        self.multihead = MultiHeadAttention(hidden_layer, n_heads)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(hidden_layer, expansion_factor*hidden_layer),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(expansion_factor*hidden_layer, hidden_layer)\n",
    "        )\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_layer)\n",
    "        self.norm2 = nn.LayerNorm(hidden_layer)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_value)\n",
    "        \n",
    "    def forward(self, z, market_index):\n",
    "        h_m = self.mlcontext(z, market_index)\n",
    "        scores = self.multihead(h_m, h_m, h_m, trans=False)[1]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_value, expansion_factor = 4, n_heads = 8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                        nn.Linear(embed_dim, expansion_factor * embed_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(expansion_factor * embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_value)\n",
    "        self.dropout2 = nn.Dropout(dropout_value)\n",
    "    \n",
    "    def forward(self, key, query, value):\n",
    "        attention_out = self.attention(key, query, value)[0]\n",
    "        attention_residual_out = attention_out + query\n",
    "        norm1_out = self.dropout1(self.norm1(attention_out)) \n",
    "        \n",
    "        feed_fwd_out = self.feed_forward(norm1_out)\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n",
    "        \n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e574f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we compute the attention scores from Transformer 2, and apply them to the embedded input vectors for the Transformer2 encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_size, embed_dim, num_features, beta, num_layers=2, expansion_factor=4, n_heads=8): \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.embedding_layer = Embedding(embed_dim) \n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "        self.correlations = CorrelationsOutput(num_features, embed_dim, input_size, beta, n_heads, expansion_factor, dropout_value)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.heads = n_heads\n",
    "        self.embed = embed_dim\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, dropout_value, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, market_index):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        embed_out = self.embedding_layer(x[:,:,:,0].squeeze(dim=-1))\n",
    "        posit_out = self.positional_encoder(embed_out)\n",
    "        correl_out = self.correlations(x[:,:,:,1:], market_index).unsqueeze(1)\n",
    "\n",
    "        posit_out1 = posit_out.view(batch_size, self.heads, posit_out.size(1), posit_out.size(2), int(self.embed/self.heads)).permute(0,3,1,2,4)\n",
    "\n",
    "        product = torch.matmul(correl_out, posit_out1)\n",
    "        scores1 = F.softmax(product, dim=-1)\n",
    "        concat = scores1.permute(0,3,1,2,4).contiguous().view(batch_size, posit_out.size(1), posit_out.size(2), self.embed) \n",
    "\n",
    "        out = self.dropout(self.norm(concat))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_value, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, dropout_value, expansion_factor, n_heads)\n",
    "        \n",
    "    def forward(self, key, x, value, mask):\n",
    "        attention = self.attention(x, x, x, mask=mask)[0]\n",
    "        x = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(key, x, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we compute the attention scores from Transformer 2, and apply them to the embedded input vectors for the Transformer2 decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_input_size, embed_dim, seq_len, target_output_size, dropout_value, num_features, beta, num_layers=2, expansion_factor=4, n_heads=8): ###batch size after target output\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(embed_dim) \n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "        self.correlations = CorrelationsOutput(num_features, embed_dim, decoder_input_size, beta, n_heads, expansion_factor, dropout_value)\n",
    "        self.heads = n_heads\n",
    "        self.embed = embed_dim\n",
    "        \n",
    "        self.fst_attention = DecoderBlock(embed_dim, dropout_value, expansion_factor=4, n_heads=8)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, dropout_value, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc1_out = nn.Linear(embed_dim, 1)\n",
    "        self.fc2_out = nn.Linear(decoder_input_size, target_output_size)\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, enc_out, mask, market_index):\n",
    "        batch_size = x.size(0)\n",
    "        embed_out = self.embedding(x[:,:,:,0].squeeze(dim=-1)) \n",
    "        posit_out = self.position_embedding(embed_out)\n",
    "        correl_out = self.correlations(x[:,:,:,1:], market_index).unsqueeze(1)\n",
    "        \n",
    "        posit_out1 = posit_out.view(batch_size, self.heads, posit_out.size(1), posit_out.size(2), int(self.embed/self.heads)).permute(0,3,1,2,4)\n",
    "            \n",
    "        product = torch.matmul(correl_out, posit_out1)\n",
    "        scores1 = F.softmax(product, dim=-1)\n",
    "        concat = scores1.permute(0,3,1,2,4).contiguous().view(batch_size, posit_out.size(1), posit_out.size(2), self.embed) #32 x 10 x 512\n",
    "\n",
    "        pre_out = self.dropout(self.norm(concat))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            decod_out = layer(enc_out, pre_out, enc_out, mask=None)\n",
    "        \n",
    "        out = self.fc1_out(decod_out)\n",
    "        out = torch.squeeze(out, dim=-1)\n",
    "        out = self.fc2_out(out)\n",
    "        out = torch.squeeze(out, dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, input_size, decoder_input_size, target_output_size, seq_length, num_features, beta, num_layers=2, dropout_value=0.2, expansion_factor=4, n_heads=8): ###batch_size, after target output\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.decoder_input_size = decoder_input_size\n",
    "        \n",
    "        self.encoder = TransformerEncoder(seq_length, input_size, embed_dim, num_features, beta, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads) ###batch_size=batch_size, after embed dim\n",
    "        self.decoder = TransformerDecoder(decoder_input_size, embed_dim, seq_length, target_output_size, dropout_value, num_features, beta, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads) ###batch_size=batch_size, after target output\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(batch_size, 1, trg_len, trg_len) #returns lower triangular matrix\n",
    "        return trg_mask\n",
    "    \n",
    "    def decode(self, src, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "        \n",
    "        out = trg\n",
    "        for i in range(seq_len):\n",
    "            out = self.decoder(out, enc_out, trg_mask)\n",
    "            \n",
    "            out = out[:, -1, :]\n",
    "            \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out, axis=0)\n",
    "            \n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, market_index):\n",
    "        enc_out = self.encoder(src, market_index) \n",
    "        \n",
    "        outputs = self.decoder(src[:,:,1:,:], enc_out, 0, market_index[:,:,1:,:]) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15256046",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "decoder_input_size = 9\n",
    "target_output_size = 1\n",
    "num_layers = 6\n",
    "seq_length = 10 \n",
    "batch_size = 16\n",
    "dropout_value = 0.2\n",
    "num_training_stocks = 5\n",
    "\n",
    "num_features = 11\n",
    "beta = 0.1\n",
    "\n",
    "\n",
    "model = Transformer(embed_dim=32, input_size=input_size, \n",
    "                    decoder_input_size=decoder_input_size, target_output_size=target_output_size, seq_length=seq_length, num_features=num_features, beta = beta, \n",
    "                    num_layers=num_layers, dropout_value=dropout_value, expansion_factor=4, n_heads=8) \n",
    "\n",
    "current_path = os.path.join('')\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59feb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, inputs, labels, transform=None, target_transform=None):\n",
    "        self.labels = labels\n",
    "        self.inputs = inputs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inpt = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            inpt = self.transform(inpt)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inpt, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_path = os.path.join(current_path, \"..\", \"..\", \"Data\", \"Network 2 data\", \"Market Index data\")\n",
    "years = [f for f in listdir(MI_path) if isfile(join(MI_path, f))]\n",
    "MI_data = []\n",
    "for year in years:\n",
    "    data2 = list(reversed(open(os.path.join(MI_path, year)).read().splitlines()[1:]))\n",
    "    for i in range(len(data2)):\n",
    "        lst = data2[i].split(',\"')\n",
    "        for j in range(len(lst)-1):\n",
    "            lst[j+1] = lst[j+1][:-1]\n",
    "            if ',' in lst[j+1]:\n",
    "                idx = lst[j+1].find(',')\n",
    "                lst[j+1] = lst[j+1][:idx] + lst[j+1][idx+1:]\n",
    "        lst += ['volume', lst[-1]]\n",
    "        data2[i] = ','.join(lst)\n",
    "    MI_data+=data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d483b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dataset in the same way as for network 2\n",
    "mypath = os.path.join(current_path, \"..\", \"..\", \"Data\", \"Network 2 data\", \"Adv-ALSTM-master\", \"data\", \"kdd17\", \"price_long_50\")\n",
    "stocks = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "feature_vectors = {}\n",
    "for i in range(num_training_stocks+1):\n",
    "    if i == 0:\n",
    "        data = MI_data\n",
    "    else:\n",
    "        data = open(os.path.join(mypath, stocks[i-1])).read().splitlines()\n",
    "        data = list(reversed(data[1:]))\n",
    "    date1 = data[0].split(',')[0].replace('-', '/').split('/')\n",
    "    date2 = data[1].split(',')[0].replace('-', '/').split('/')\n",
    "    year = 0\n",
    "    month = 0\n",
    "    day_of_month = 0\n",
    "    for i in range(len(date1)):\n",
    "        if len(date1[i]) == 4:\n",
    "            year = i\n",
    "        elif date1[i] == date2[i]:\n",
    "            month = i\n",
    "        else:\n",
    "            day_of_month = i\n",
    "    \n",
    "    previous_adjclose = []\n",
    "    previous_close = 0\n",
    "    previous_date = []        \n",
    "    closing_prices = []\n",
    "    \n",
    "    for day in data:\n",
    "        lst = day.split(',')\n",
    "        close_price = float(lst[4])\n",
    "        closing_prices.append(close_price)\n",
    "    \n",
    "    mean = np.mean(closing_prices)\n",
    "    means.append(mean)\n",
    "    std = np.std(closing_prices)\n",
    "    stds.append(std)\n",
    "    \n",
    "    for day in data[:30]: \n",
    "        lst = day.split(',')\n",
    "        adj_close = float(lst[6])\n",
    "        previous_adjclose.append(adj_close)\n",
    "        if day == data[29]:\n",
    "            previous_close = float(lst[4]) \n",
    "    for day in data[31:]:    \n",
    "        lst = day.split(',')\n",
    "        date_lst = lst[0].replace('-', '/').split('/')   \n",
    "        date = '/'.join([str(int(date_lst[day_of_month])), str(int(date_lst[month])), str(int(date_lst[year]))])\n",
    "       \n",
    "        open_price = float(lst[1])\n",
    "        close_price = float(lst[4])\n",
    "        high = float(lst[2])\n",
    "        low = float(lst[3])\n",
    "        adj_close = float(lst[6])\n",
    "        feature_vector = [float((close_price-mean)/std), (open_price/close_price)-1, (high/close_price)-1, (low/close_price)-1, (close_price/previous_close)-1, (adj_close/previous_adjclose[-1])-1]\n",
    "        for i in range(5, 35, 5):\n",
    "            feature_vector.append((sum(previous_adjclose[-i:])/(i*adj_close))-1)\n",
    "        if date not in feature_vectors.keys():\n",
    "            feature_vectors[date] = [feature_vector]\n",
    "        else:\n",
    "            feature_vectors[date].append(feature_vector)\n",
    "        previous_close = close_price\n",
    "        previous_adjclose.append(adj_close)\n",
    "\n",
    "\n",
    "year_lookup = {} \n",
    "for date in feature_vectors.keys():\n",
    "    \n",
    "    if len(feature_vectors[date]) != num_training_stocks+1:\n",
    "        continue\n",
    "    else:\n",
    "        date_lst = date.split('/')\n",
    "        year = int(date_lst[2])\n",
    "        if year not in year_lookup.keys():\n",
    "            year_lookup[year] = [feature_vectors[date]]\n",
    "        else:\n",
    "            year_lookup[year].append(feature_vectors[date])\n",
    "\n",
    "train_feature_vectors = []\n",
    "val_feature_vectors = []\n",
    "test_feature_vectors = []\n",
    "years = sorted(list(year_lookup.keys()))\n",
    "\n",
    "for year in range(years[-2], years[-1]+1):\n",
    "    test_feature_vectors += year_lookup[year]\n",
    "training_cut_off = years[math.ceil(0.75*len(years))-1]\n",
    "for year in range(years[0], training_cut_off):\n",
    "    train_feature_vectors += year_lookup[year]\n",
    "for year in range(training_cut_off, years[-1]):\n",
    "    val_feature_vectors += year_lookup[year]\n",
    "    \n",
    "\n",
    "block_size = input_size\n",
    "def build_dataset(feature_vectors):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(feature_vectors)-block_size):\n",
    "        X.append(feature_vectors[i:i+block_size])\n",
    "        day = feature_vectors[i+block_size][1:]\n",
    "        Y_builder = []\n",
    "        for j in range(len(day)):\n",
    "            Y_builder.append(day[j][0])\n",
    "        Y.append(Y_builder)\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "Xtr, Ytr = build_dataset(train_feature_vectors)\n",
    "Xval, Yval = build_dataset(val_feature_vectors)\n",
    "Xte, Yte = build_dataset(test_feature_vectors)\n",
    "\n",
    "training_data = CustomDataset(Xtr, Ytr)\n",
    "val_data = CustomDataset(Xval, Yval)\n",
    "\n",
    "trainloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = CustomDataset(Xte, Yte)\n",
    "testloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e18fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "for e in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    tot_train_loss = 0\n",
    "    model.train()\n",
    "    for x, y in trainloader:\n",
    "        if len(y) != batch_size:\n",
    "            continue\n",
    "        z = torch.transpose(x, 1, 2)\n",
    "        market_index = z[:,0,:,1:]\n",
    "        market_index = torch.unsqueeze(market_index, 1)\n",
    "        z = z[:, 1:]     \n",
    "           \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(z, market_index)\n",
    "        loss = criterion(outputs, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_train_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        tot_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, y in valloader:\n",
    "                if len(y) != batch_size:\n",
    "                    continue\n",
    "                z = torch.transpose(x, 1, 2)\n",
    "                market_index = z[:,0,:,1:]\n",
    "                market_index = torch.unsqueeze(market_index, 1)\n",
    "                z = z[:, 1:]\n",
    "                outputs = model(z, market_index)\n",
    "                loss = criterion(outputs, y)\n",
    "                tot_val_loss += loss.item()\n",
    "\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        val_loss = tot_val_loss / len(valloader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}/{}.. \".format(e, epochs),\n",
    "              \"Training Loss: {:.5f}.. \".format(train_loss),\n",
    "              \"Test Loss: {:.5f}.. \".format(val_loss), \n",
    "              \"Time taken: {:.3f}..\".format(time.time()-start_time))\n",
    "            \n",
    "    if e%10 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(current_path, \"Checkpoints\", 'checkpoint' + str(e) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40481b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#best checkpoint finder\n",
    "best_checkpoint = 0\n",
    "losses = {}\n",
    "scores = {}\n",
    "for c in range(1, 151):\n",
    "    print(c)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        checkpoint = 10*c\n",
    "        state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", 'checkpoint' + str(checkpoint) + '.pth'))\n",
    "        model.load_state_dict(state_dict)\n",
    "        inpts = test_data[0][0][:,1:6,0]\n",
    "        outs = test_data[0][0][:,1:6,0]\n",
    "        for x, y in testloader:\n",
    "\n",
    "            z = torch.transpose(x, 1, 2)\n",
    "            market_index = z[:,0,:,1:]\n",
    "            market_index = torch.unsqueeze(market_index, 1)\n",
    "            z = z[:, 1:]     \n",
    "\n",
    "            outputs = model(z, market_index)\n",
    "\n",
    "            outs = torch.cat((outs,outputs))\n",
    "\n",
    "            inpts = torch.cat((inpts, y))\n",
    "\n",
    "    prods = []\n",
    "    mses = []\n",
    "    for s in range(5):\n",
    "        inpt = inpts[:,s]\n",
    "        out = outs[:,s]\n",
    "\n",
    "        prod1 = 1\n",
    "        for j in range(1, len(inpt)):\n",
    "            sign = 1\n",
    "            if out[j]-out[j-1] < 0:\n",
    "                sign = -1\n",
    "            prod1 *= (1+((inpt[j]-inpt[j-1])/inpt[j-1]*sign))\n",
    "        prods.append(prod1)\n",
    "\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for j in range(len(inpt)-10):\n",
    "            total_loss += (inpt[j+10]-out[j+10])**2\n",
    "        total_loss/=(len(inpt)-10)\n",
    "        mses.append(total_loss)\n",
    "\n",
    "    losses[checkpoint] = np.mean(mses)\n",
    "    \n",
    "    scores[checkpoint] = np.mean(prods)\n",
    "    \n",
    "sorted_losses = dict(sorted(losses.items(), key=lambda item: item[1]))\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "good_checkpoints = []\n",
    "threshold = min(losses.values()) + 2\n",
    "\n",
    "for check in losses.keys():\n",
    "    if losses[check] < threshold:\n",
    "        good_checkpoints.append(check)\n",
    "\n",
    "for check in sorted_scores.keys():\n",
    "    if check in good_checkpoints:\n",
    "        best_checkpoint = check\n",
    "        break\n",
    "\n",
    "bag_checkpoints = []\n",
    "for check in sorted_scores.keys():\n",
    "    if check in good_checkpoints:\n",
    "        bag_checkpoints.append(check)\n",
    "\n",
    "print(best_checkpoint, bag_checkpoints)\n",
    "print(sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a5f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test for 1 stock\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "checkpoint = 1070\n",
    "state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", 'checkpoint' + str(checkpoint) + '.pth'))\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "    model.eval()    \n",
    "    inpt = [x[3][0] for x in test_data[0][0]]\n",
    "    out = [x[3][0] for x in test_data[0][0]]\n",
    "    for x, y in testloader:\n",
    "        \n",
    "        z = torch.transpose(x, 1, 2)\n",
    "        market_index = z[:,0,:,1:]\n",
    "        market_index = torch.unsqueeze(market_index, 1)\n",
    "        z = z[:, 1:]     \n",
    "                   \n",
    "        outputs = model(z, market_index)\n",
    "        \n",
    "        out.append(outputs.squeeze()[2])\n",
    "        inpt.append(y.squeeze()[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inpt = [inp*stds[3]+means[3] for inp in inpt]\n",
    "out = [ot*stds[3]+means[3] in out]\n",
    "\n",
    "prod1 = 1\n",
    "for i in range(1, len(inpt)):\n",
    "    sign = 1\n",
    "    if out[i]-out[i-1] < 0:\n",
    "        sign = -1\n",
    "    prod1 *= (1+(inpt[i]-inpt[i-1])/inpt[i-1]*sign)\n",
    "\n",
    "total_loss = 0.0\n",
    "for i in range(len(inpt)-10):\n",
    "    outpt = out[i+10].unsqueeze(0)\n",
    "    total_loss += (inpt[i+10]-out[i+10])**2\n",
    "total_loss/=(len(inpt)-10)\n",
    "\n",
    "\n",
    "print(prod1, total_loss)\n",
    "\n",
    "plt.plot(inpt, label='real data')\n",
    "plt.plot(out, alpha=0.5, label='predicted values')\n",
    "\n",
    "plt.xlabel('Days', fontsize=15)\n",
    "plt.ylabel('Closing Price', fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f51b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging test for all 5 stocks\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "inpts = []\n",
    "outs = []\n",
    "for checkpoint in [960, 850, 1190, 1030, 780, 1390, 990, 1240, 1000, 1070]:\n",
    "    state_dict = torch.load(os.path.join(current_path, \"Checkpoints\", 'checkpoint' + str(checkpoint) + '.pth'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        inpt = test_data[0][0][:,1:6,0]\n",
    "        out = test_data[0][0][:,1:6,0]\n",
    "        for x, y in testloader:\n",
    "\n",
    "            z = torch.transpose(x, 1, 2)\n",
    "            market_index = z[:,0,:,1:]\n",
    "            market_index = torch.unsqueeze(market_index, 1)\n",
    "            z = z[:, 1:]     \n",
    "\n",
    "            outputs = model(z, market_index)\n",
    "\n",
    "            out = torch.cat((out,outputs))\n",
    "\n",
    "            inpt = torch.cat((inpt, y))\n",
    "        inpts.append(inpt)\n",
    "        outs.append(out)\n",
    "        \n",
    "inpts = torch.stack(inpts)\n",
    "outs = torch.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate results for all 5 stocks\n",
    "inpts2 = torch.mean(inpts, dim=0) + 10 # +10 to avoid dividing by 0 in prod1\n",
    "outs2 = torch.mean(outs, dim=0) + 10\n",
    "prods = []\n",
    "mses = []\n",
    "\n",
    "for s in range(5):\n",
    "    inpt = inpts2[:,s]\n",
    "    out = outs2[:,s]\n",
    "    \n",
    "    prod1 = 1\n",
    "    for j in range(1, len(inpt)):\n",
    "        sign = 1\n",
    "        if out[j]-out[j-1] < 0:\n",
    "            sign = -1\n",
    "        prod1 *= (1+((inpt[j]-inpt[j-1])/inpt[j-1]*sign))\n",
    "    prods.append(prod1)\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for j in range(len(inpt)-10):\n",
    "        total_loss += (inpt[j+10]-out[j+10])**2\n",
    "    total_loss/=(len(inpt)-10)\n",
    "    mses.append(total_loss)\n",
    "\n",
    "\n",
    "    print(prod1, prod2, total_loss, total_loss2)\n",
    "\n",
    "    plt.plot(inpt, label='real data')\n",
    "    plt.plot(out, alpha=0.5, label='predicted values')\n",
    "\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print(prods, mses, np.mean(prods), np.mean(mses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
